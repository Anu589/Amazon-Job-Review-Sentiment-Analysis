{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anu589/Amazon-Job-Review-Sentiment-Analysis/blob/main/Copy_of_Amazon_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U kaleido"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGPUWXWPHF0M",
        "outputId": "91b3640a-ffcd-4cc2-cf10-94010e585ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaleido\n",
            "  Downloading kaleido-1.1.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting choreographer>=1.0.10 (from kaleido)\n",
            "  Downloading choreographer-1.1.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting logistro>=1.0.8 (from kaleido)\n",
            "  Downloading logistro-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.12/dist-packages (from kaleido) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kaleido) (25.0)\n",
            "Collecting pytest-timeout>=2.4.0 (from kaleido)\n",
            "  Downloading pytest_timeout-2.4.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.12/dist-packages (from choreographer>=1.0.10->kaleido) (3.20.1)\n",
            "Requirement already satisfied: pytest>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from pytest-timeout>=2.4.0->kaleido) (8.4.2)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.19.2)\n",
            "Downloading kaleido-1.1.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading choreographer-1.1.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading logistro-1.1.0-py3-none-any.whl (7.9 kB)\n",
            "Downloading pytest_timeout-2.4.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: logistro, pytest-timeout, choreographer, kaleido\n",
            "Successfully installed choreographer-1.1.1 kaleido-1.1.0 logistro-1.1.0 pytest-timeout-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqDy7-woUv_8",
        "outputId": "67031fa0-803b-4997-ee6d-1ab8cae18e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. Project Setup and Imports\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from collections import Counter\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.extra.rate_limiter import RateLimiter\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import os\n",
        "from tqdm.autonotebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "import kaleido\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def save_chart(fig, filename):\n",
        "    \"\"\"\n",
        "    Saves a figure to the 'charts' directory.\n",
        "    - Plotly figures are saved as interactive HTML files.\n",
        "    - Matplotlib figures are saved as static PNG images.\n",
        "    \"\"\"\n",
        "    if not os.path.exists('charts'):\n",
        "        os.makedirs('charts')\n",
        "    if isinstance(fig, go.Figure):\n",
        "        fig.write_html(f\"charts/{filename}.html\")\n",
        "        print(f\"Saved chart to charts/{filename}.html\")\n",
        "    else:\n",
        "        fig.savefig(f\"charts/{filename}.png\")\n",
        "        print(f\"Saved chart to charts/{filename}.png\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. LLM Model and Vectorized Sentiment Analysis\n",
        "# ==============================================================================\n",
        "def load_llama_model():\n",
        "    \"\"\"\n",
        "    Loads a Hugging Face T5 model and tokenizer for sentiment analysis.\n",
        "    \"\"\"\n",
        "    model_name = \"google/flan-t5-small\"\n",
        "    try:\n",
        "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading LLM: {e}\")\n",
        "        return None, None\n",
        "\n",
        "model, tokenizer = load_llama_model()\n",
        "\n",
        "def get_llama_sentiment(text_list, batch_size=32):\n",
        "    \"\"\"\n",
        "    Analyzes a list of texts for sentiment using the T5 model with batching for efficiency.\n",
        "    Returns a list of sentiment scores (-1, 0, 1).\n",
        "    \"\"\"\n",
        "    if model is None or tokenizer is None:\n",
        "        print(\"LLM not loaded. Skipping sentiment analysis.\")\n",
        "        return [0] * len(text_list)\n",
        "\n",
        "    sentiments = []\n",
        "    # This tqdm bar will now only show 32 batches (1000/32)\n",
        "    for i in tqdm(range(0, len(text_list), batch_size), desc=\"Getting Llama Sentiments (Batched)\"):\n",
        "        batch = text_list[i:i + batch_size]\n",
        "\n",
        "        prompts = [\n",
        "            f\"What is the sentiment of this review? ' {text} '. Answer with a single word: Positive, Negative, or Neutral.\"\n",
        "            if isinstance(text, str) and text.strip() not in [\"na\", \"nan\", \"none\", \"#name?\", \"no text provided\"]\n",
        "            else None for text in batch\n",
        "        ]\n",
        "\n",
        "        valid_prompts = [p for p in prompts if p is not None]\n",
        "        if not valid_prompts:\n",
        "            sentiments.extend([0] * len(batch))\n",
        "            continue\n",
        "\n",
        "        inputs = tokenizer(valid_prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
        "        sentiment_labels = [tokenizer.decode(output, skip_special_tokens=True).strip().lower() for output in outputs]\n",
        "\n",
        "        sentiment_idx = 0\n",
        "        for prompt in prompts:\n",
        "            if prompt is None:\n",
        "                sentiments.append(0)\n",
        "            else:\n",
        "                label = sentiment_labels[sentiment_idx]\n",
        "                if \"positive\" in label:\n",
        "                    sentiments.append(1)\n",
        "                elif \"negative\" in label:\n",
        "                    sentiments.append(-1)\n",
        "                else:\n",
        "                    sentiments.append(0)\n",
        "                sentiment_idx += 1\n",
        "\n",
        "    return sentiments\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Data Loading, Preprocessing & Feature Engineering\n",
        "# ==============================================================================\n",
        "def load_and_preprocess_data():\n",
        "    \"\"\"\n",
        "    Loads and cleans the Amazon review and world cities datasets,\n",
        "    taking a 1000-row sample for faster processing.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        reviews_data = pd.read_csv(\"Amazon_job_reviews_(USA_ India)2008-2020.csv\")\n",
        "        world_cities = pd.read_csv(\"worldcities.csv\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}. Make sure the CSV files are in the same directory.\")\n",
        "        return None, None\n",
        "\n",
        "\"\"\"    # --- CRITICAL FIX: SAMPLE DATA HERE BEFORE ANY OTHER PROCESSING ---\n",
        "    if len(reviews_data) > 1000:\n",
        "        reviews_data = reviews_data.sample(n=1000, random_state=42)\n",
        "        print(f\"\\nSampling complete: Using 1000 rows for the entire analysis pipeline.\")\"\"\"\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    # Clean column names\n",
        "    reviews_data.columns = reviews_data.columns.str.strip()\n",
        "\n",
        "    print(\"--- Raw Data Insights (Sampled) ---\")\n",
        "    print(\"Shape of the reviews data:\", reviews_data.shape)\n",
        "    print(\"\\nFirst 5 rows of the data:\")\n",
        "    print(reviews_data.head())\n",
        "\n",
        "    # FIX 2: Correct Date Format ('%y') and drop critical NaNs\n",
        "    reviews_data['Date'] = pd.to_datetime(reviews_data['Date'], format=\"%d %b %y\", errors='coerce')\n",
        "    reviews_data.dropna(subset=['Date', 'Location', 'Position', 'Overall rating'], inplace=True)\n",
        "    reviews_data['Year'] = reviews_data['Date'].dt.year.astype(int)\n",
        "\n",
        "    # Feature Engineering\n",
        "    reviews_data['is_current_employee'] = reviews_data['Current employee'].apply(lambda x: 1 if x == 'Current employee' else 0)\n",
        "\n",
        "    # Define columns for robust cleaning\n",
        "    numerical_rating_cols = ['Work/Life Balance', 'Career Opportunities', 'Compensation and Benefits', 'Senior Management']\n",
        "    zero_fill_cols = ['Culture and Values', 'Diversity and Inclusion']\n",
        "    categorical_cols = ['CEO Approval', 'Recommended', 'Business Outlook']\n",
        "    error_strings = ['#NAME?', '?', 'n/a', 'none', 'nil', ' ']\n",
        "\n",
        "    # 3. ROBUST NUMERICAL IMPUTATION (Mean Filling)\n",
        "    for col in numerical_rating_cols:\n",
        "        reviews_data[col] = reviews_data[col].fillna(reviews_data[col].mean())\n",
        "\n",
        "    # 4. ROBUST ZERO IMPUTATION\n",
        "    for col in zero_fill_cols:\n",
        "        reviews_data[col] = pd.to_numeric(reviews_data[col], errors='coerce')\n",
        "        reviews_data[col] = reviews_data[col].fillna(0)\n",
        "\n",
        "    # 5. ROBUST CATEGORICAL IMPUTATION\n",
        "    for col in categorical_cols:\n",
        "        reviews_data[col] = reviews_data[col].replace('nan', np.nan)\n",
        "        reviews_data[col] = reviews_data[col].replace(error_strings, 'unknown')\n",
        "        reviews_data[col] = reviews_data[col].fillna('unknown')\n",
        "\n",
        "    # Drop Timeline column\n",
        "    reviews_data.drop('Timeline', axis=1, inplace=True)\n",
        "\n",
        "    # Handle text columns and fill with explicit placeholders\n",
        "    reviews_data['pros'] = reviews_data['pros'].fillna(\"no text provided\").str.lower()\n",
        "    reviews_data['cons'] = reviews_data['cons'].fillna(\"no text provided\").str.lower()\n",
        "\n",
        "    reviews_data['Comment for company'] = reviews_data['Comment for company'].replace(error_strings, 'no comment provided')\n",
        "    reviews_data['Comment for company'] = reviews_data['Comment for company'].fillna('no comment provided').str.lower()\n",
        "\n",
        "    reviews_data['advice to Management'] = reviews_data['advice to Management'].replace(error_strings, 'no advice provided')\n",
        "    reviews_data['advice to Management'] = reviews_data['advice to Management'].fillna('no advice provided').str.lower()\n",
        "\n",
        "    # Encode categorical columns\n",
        "    reviews_data['CEO_Approval_Score'] = reviews_data['CEO Approval'].map({'yes': 1, 'may be': 0.5, 'no': 0, 'unknown': 0.5})\n",
        "    reviews_data['Recommended_Score'] = reviews_data['Recommended'].map({'yes': 1, 'may be': 0.5, 'no': 0, 'unknown': 0.5})\n",
        "    reviews_data['Business_Outlook_Score'] = reviews_data['Business Outlook'].map({'yes': 1, 'may be': 0.5, 'no': 0, 'unknown': 0.5})\n",
        "\n",
        "    reviews_data.info()\n",
        "\n",
        "    return reviews_data, world_cities\n",
        "\n",
        "\n",
        "def generate_wordcloud(text_series, title, filename):\n",
        "    \"\"\"Generates and saves a word cloud from a pandas Series of text.\"\"\"\n",
        "    text = \" \".join(review for review in text_series if isinstance(review, str) and review.strip() not in [\"na\", \"nan\", \"none\", \"#name?\"])\n",
        "    if not text:\n",
        "        print(f\"No valid text available for the {title} word cloud.\")\n",
        "        return\n",
        "    stopwords = [\"the\", \"and\", \"a\", \"to\", \"of\", \"is\", \"for\", \"in\", \"it\", \"with\", \"be\", \"not\", \"on\", \"that\", \"i\", \"you\", \"they\", \"we\", \"pros\", \"cons\", \"amazon\", \"company\", \"work\", \"job\", \"good\", \"great\", \"nice\", \"time\", \"people\"]\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text)\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.set_title(title, fontsize=20)\n",
        "    ax.axis('off')\n",
        "    save_chart(fig, filename)\n",
        "\n",
        "\n",
        "def perform_analysis(reviews_data, world_cities):\n",
        "    \"\"\"\n",
        "    Performs all analysis steps and generates charts.\n",
        "    Returns the processed reviews_data DataFrame and the correlation DataFrame.\n",
        "    \"\"\"\n",
        "    print(\"Starting analysis and visualizations...\")\n",
        "\n",
        "    reviews_data.dropna(subset=['Year'], inplace=True)\n",
        "    reviews_data['Year'] = reviews_data['Year'].astype(int)\n",
        "\n",
        "    # All subsequent LLM calls run on the 1000-row sample\n",
        "    print(\"\\nPerforming Sentiment Analysis with LLM...\")\n",
        "    reviews_data['pros_sentiment'] = get_llama_sentiment(reviews_data['pros'].tolist())\n",
        "    reviews_data['cons_sentiment'] = get_llama_sentiment(reviews_data['cons'].tolist())\n",
        "    reviews_data['comment_sentiment'] = get_llama_sentiment(reviews_data['Comment for company'].tolist())\n",
        "    reviews_data['advice_sentiment'] = get_llama_sentiment(reviews_data['advice to Management'].tolist())\n",
        "    reviews_data['overall_satisfaction'] = reviews_data[['pros_sentiment', 'comment_sentiment']].mean(axis=1)\n",
        "\n",
        "    print(\"\\n--- After LLM Sentiment Analysis ---\")\n",
        "    print(\"New sentiment columns added:\")\n",
        "    print(reviews_data[['pros', 'pros_sentiment', 'cons_sentiment', 'comment_sentiment']].head())\n",
        "    print(\"\\n------------------------------------\")\n",
        "\n",
        "    print(\"\\nGenerating Word Clouds...\")\n",
        "    generate_wordcloud(reviews_data['pros'], \"Most Frequent Pros\", \"pros_wordcloud\")\n",
        "    generate_wordcloud(reviews_data['cons'], \"Most Frequent Cons\", \"cons_wordcloud\")\n",
        "\n",
        "    print(\"\\nGenerating Employee Location Map...\")\n",
        "    def get_coordinates():\n",
        "        geolocator = Nominatim(user_agent=\"amazon_reviews_app\")\n",
        "        geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
        "        employee_count_by_city = reviews_data['Location'].value_counts().reset_index()\n",
        "        employee_count_by_city.columns = ['City_Name', 'Employee_Count']\n",
        "        matched_data = pd.merge(employee_count_by_city, world_cities[['city_ascii', 'lat', 'lng']],\n",
        "                                left_on='City_Name', right_on='city_ascii', how='left')\n",
        "        matched_data.dropna(subset=['lat', 'lng'], inplace=True)\n",
        "        return matched_data\n",
        "    map_data = get_coordinates()\n",
        "    fig = px.scatter_geo(map_data, lat='lat', lon='lng', color=\"Employee_Count\",\n",
        "                         hover_name=\"City_Name\", size=\"Employee_Count\",\n",
        "                         projection=\"natural earth\", title=\"Employee Locations (USA and India)\")\n",
        "    save_chart(fig, \"employee_map\")\n",
        "\n",
        "    print(\"\\nGenerating Overall Satisfaction Histogram...\")\n",
        "    reviews_data['sentiment_category'] = reviews_data['overall_satisfaction'].apply(\n",
        "        lambda x: 'Satisfied' if x > 0 else ('Dissatisfied' if x < 0 else 'Neutral')\n",
        "    )\n",
        "    sentiment_counts = reviews_data.groupby(['Year', 'sentiment_category']).size().unstack(fill_value=0)\n",
        "    sentiment_counts = sentiment_counts.reindex(columns=['Satisfied', 'Dissatisfied', 'Neutral'], fill_value=0)\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Bar(x=sentiment_counts.index, y=sentiment_counts['Satisfied'], name='Satisfied', marker_color='lightgreen'))\n",
        "    fig.add_trace(go.Bar(x=sentiment_counts.index, y=sentiment_counts['Dissatisfied'], name='Dissatisfied', marker_color='salmon'))\n",
        "    fig.add_trace(go.Bar(x=sentiment_counts.index, y=sentiment_counts['Neutral'], name='Neutral', marker_color='lightgrey'))\n",
        "    fig.update_layout(barmode='stack', title='Overall Employee Satisfaction by Year', xaxis_title='Year', yaxis_title='Number of Reviews')\n",
        "    fig.update_xaxes(dtick=1)\n",
        "    save_chart(fig, \"satisfaction_histogram\")\n",
        "\n",
        "    print(\"\\nGenerating Sentiment Analysis Over Time Chart...\")\n",
        "    sentiment_summary = reviews_data.groupby('Year').agg(\n",
        "        avg_pros=('pros_sentiment', 'mean'),\n",
        "        avg_cons=('cons_sentiment', 'mean'),\n",
        "        avg_comment=('comment_sentiment', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Bar(x=sentiment_summary['Year'], y=sentiment_summary['avg_pros'], name='Pros Sentiment', marker_color='lightgreen'))\n",
        "    fig.add_trace(go.Bar(x=sentiment_summary['Year'], y=sentiment_summary['avg_cons'], name='Cons Sentiment', marker_color='salmon'))\n",
        "    fig.add_trace(go.Scatter(x=sentiment_summary['Year'], y=sentiment_summary['avg_comment'], mode='lines+markers', name='Comment for Company Trend', line=dict(color='grey', width=2)))\n",
        "    fig.update_layout(title_text=\"Sentiment Analysis: Pros, Cons, and Comments Over Time\", barmode='group')\n",
        "    fig.update_xaxes(dtick=1)\n",
        "    save_chart(fig, \"sentiment_analysis\")\n",
        "\n",
        "    print(\"\\nGenerating Ratings Impact Line Chart...\")\n",
        "    rating_columns = ['Work/Life Balance', 'Culture and Values', 'Diversity and Inclusion',\n",
        "                      'Career Opportunities', 'Compensation and Benefits', 'Senior Management']\n",
        "    yearly_correlations = []\n",
        "    for year in reviews_data['Year'].unique():\n",
        "        year_data = reviews_data[reviews_data['Year'] == year].dropna(subset=['Overall rating'] + rating_columns)\n",
        "        if year_data.shape[0] > 1:\n",
        "            for col in rating_columns:\n",
        "                corr = year_data['Overall rating'].corr(year_data[col])\n",
        "                yearly_correlations.append({'Year': year, 'Rating_Type': col, 'Correlation': corr})\n",
        "    corr_df = pd.DataFrame(yearly_correlations)\n",
        "\n",
        "    if corr_df.empty:\n",
        "        print(\"Warning: Not enough valid data to generate 'Ratings Impact Line Chart'. Skipping.\")\n",
        "    else:\n",
        "        fig = px.line(corr_df, x='Year', y='Correlation', color='Rating_Type', title='Impact of Ratings on Overall Rating Over Time')\n",
        "        fig.update_xaxes(dtick=1)\n",
        "        save_chart(fig, \"ratings_impact_line_chart\")\n",
        "\n",
        "    print(\"\\nGenerating Organizational Health Radar Chart...\")\n",
        "    avg_scores = reviews_data.groupby('Year')[['CEO_Approval_Score', 'Recommended_Score', 'Business_Outlook_Score']].mean().reset_index()\n",
        "    fig = go.Figure()\n",
        "    for year in avg_scores['Year'].unique():\n",
        "        year_data = avg_scores[avg_scores['Year'] == year]\n",
        "        fig.add_trace(go.Scatterpolar(\n",
        "            r=year_data[['CEO_Approval_Score', 'Recommended_Score', 'Business_Outlook_Score']].values[0],\n",
        "            theta=['CEO Approval', 'Recommended', 'Business Outlook'],\n",
        "            fill='toself',\n",
        "            name=str(year)\n",
        "        ))\n",
        "    fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])),\n",
        "                      title=\"Organizational Health by Year\")\n",
        "    fig.update_xaxes(dtick=1)\n",
        "    save_chart(fig, \"organizational_health_radar_chart\")\n",
        "\n",
        "    print(\"\\nGenerating LLM-based Advice Summary...\")\n",
        "    yearly_advice = reviews_data.groupby('Year')['advice to Management'].apply(\n",
        "        lambda x: \" \".join(review for review in x if review != \"no text provided\")\n",
        "    ).to_dict()\n",
        "\n",
        "    for year, advice in yearly_advice.items():\n",
        "        if len(advice) > 100:\n",
        "            prompt = f\"Summarize the following employee advice to management for the year {year}:\\n\\n{advice}\"\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            outputs = model.generate(**inputs, max_length=150, min_length=40)\n",
        "            summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            print(f\"\\n--- Summary for {year} ---\")\n",
        "            print(summary)\n",
        "            with open(f\"charts/advice_summary_{year}.txt\", \"w\") as f:\n",
        "                f.write(summary)\n",
        "        else:\n",
        "            print(f\"\\nNot enough data to summarize for {year}.\")\n",
        "\n",
        "    return reviews_data, corr_df\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. Predictive Modeling and Evaluation\n",
        "# ==============================================================================\n",
        "def run_predictive_model(reviews_data):\n",
        "    \"\"\"\n",
        "    Performs the full predictive modeling pipeline to predict employee churn.\n",
        "\n",
        "    This includes:\n",
        "    - Feature selection\n",
        "    - Data splitting (train/test)\n",
        "    - Conditional handling of class imbalance (SMOTE)\n",
        "    - Training a RandomForestClassifier\n",
        "    - Evaluating the model's performance with key metrics and visualizations\n",
        "    - Predicting the probability of an employee staying\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Predictive Modeling for Employee Churn ---\")\n",
        "\n",
        "    features = ['Overall rating', 'Work/Life Balance', 'Culture and Values', 'Diversity and Inclusion',\n",
        "                'Career Opportunities', 'Compensation and Benefits', 'Senior Management',\n",
        "                'CEO_Approval_Score', 'Recommended_Score', 'Business_Outlook_Score',\n",
        "                'pros_sentiment', 'cons_sentiment', 'comment_sentiment', 'advice_sentiment']\n",
        "    target = 'is_current_employee'\n",
        "\n",
        "    # Filter out any rows with NaN in the feature columns\n",
        "    # The dataset size is already limited to ~1000 rows by load_and_preprocess_data\n",
        "    model_data = reviews_data.dropna(subset=features + [target])\n",
        "\n",
        "    X = model_data[features]\n",
        "    y = model_data[target]\n",
        "\n",
        "    # Split the data into training and test sets (70/30 split of the 1000-row sample)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    print(f\"\\nOriginal training set size: {X_train.shape}\")\n",
        "    print(\"Original class distribution:\", Counter(y_train))\n",
        "\n",
        "    # --- CRITICAL FIX: CONDITIONAL SMOTE APPLICATION ---\n",
        "    if y_train.nunique() <= 1:\n",
        "        print(\"--- WARNING: SMOTE Skipped ---\")\n",
        "        print(\"Training skipped SMOTE as only one class was found in the training sample.\")\n",
        "        X_train_resampled = X_train\n",
        "        y_train_resampled = y_train\n",
        "    else:\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "        print(\"Resampled training set size:\", X_train_resampled.shape)\n",
        "        print(\"Resampled class distribution:\", Counter(y_train_resampled))\n",
        "\n",
        "    # Initialize and train the Random Forest Classifier\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "    # Make predictions on the original test set\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "    # Predict the probability of an employee staying (class 1)\n",
        "    y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"\\n--- Model Performance Metrics on Test Set ---\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Former Employee', 'Current Employee']))\n",
        "\n",
        "    # Plot the Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Former Employee', 'Current Employee'],\n",
        "                yticklabels=['Former Employee', 'Current Employee'], ax=ax)\n",
        "    ax.set_title(\"Confusion Matrix for Employee Churn Prediction\")\n",
        "    ax.set_xlabel(\"Predicted Label\")\n",
        "    ax.set_ylabel(\"True Label\")\n",
        "    save_chart(fig, \"confusion_matrix\")\n",
        "\n",
        "    # Feature Importance\n",
        "    feature_importances = pd.Series(rf_classifier.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.barplot(x=feature_importances, y=feature_importances.index, ax=ax, palette='viridis')\n",
        "    ax.set_title(\"Feature Importance for Employee Churn Prediction\")\n",
        "    ax.set_xlabel(\"Importance\")\n",
        "    ax.set_ylabel(\"Feature\")\n",
        "    save_chart(fig, \"feature_importance\")\n",
        "\n",
        "    print(\"\\nPredictive modeling complete. Check the 'charts' directory for evaluation visuals.\")\n",
        "    print(\"\\nExample predictions (first 5 test samples):\")\n",
        "    for i in range(5):\n",
        "        print(f\"Sample {i+1}: True Label={y_test.iloc[i]}, Predicted Label={y_pred[i]}, Predicted Probability of Staying={y_pred_proba[i]:.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. Main Execution Block\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    reviews_data, world_cities = load_and_preprocess_data()\n",
        "    if reviews_data is not None and world_cities is not None:\n",
        "        reviews_data_processed, corr_df = perform_analysis(reviews_data, world_cities)\n",
        "        print(\"\\n\\n--- FINAL DATA PREVIEW BEFORE MODEL TRAINING ---\")\n",
        "        print(\"Final DataFrame with all features (Head):\")\n",
        "        print(reviews_data_processed.head())\n",
        "        print(\"\\nCorrelations DataFrame (Head):\")\n",
        "        print(corr_df.head())\n",
        "        print(\"\\n------------------------------------------------\")\n",
        "        run_predictive_model(reviews_data_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1317c132daea47eda66b21ca0fd508b0",
            "3a0c73c2a6be4ae5a0cbd9956abdf7c4",
            "7a9d2fd1b9ff4faaaf6a331d14cb2113",
            "df69266ee9354ffeb07df049f9dbac43",
            "07043dc2970e4aa7804b9a7e3d27e282",
            "6602c24639d946a284e02b125add9424",
            "27fd4f9c7f7c495d802452df75dfd403",
            "0838929f75ff402d9d39cbf1e6cadc80",
            "e13f7c990c9b4f87a161247d68cb38b8"
          ]
        },
        "id": "Xd8MM_MXde2C",
        "outputId": "eca20828-c33f-4d50-8c57-bb5fa6cbd226"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/kaleido/_sync_server.py:11: UserWarning: \n",
            "\n",
            "Warning: You have Plotly version 5.24.1, which is not compatible with this version of Kaleido (1.1.0).\n",
            "\n",
            "This means that static image generation (e.g. `fig.write_image()`) will not work.\n",
            "\n",
            "Please upgrade Plotly to version 6.1.1 or greater, or downgrade Kaleido to version 0.2.1.\n",
            "\n",
            "  from .kaleido import Kaleido\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1317c132daea47eda66b21ca0fd508b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a0c73c2a6be4ae5a0cbd9956abdf7c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a9d2fd1b9ff4faaaf6a331d14cb2113",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df69266ee9354ffeb07df049f9dbac43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07043dc2970e4aa7804b9a7e3d27e282",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6602c24639d946a284e02b125add9424",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27fd4f9c7f7c495d802452df75dfd403",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Raw Data Insights (Sampled) ---\n",
            "Shape of the reviews data: (29494, 22)\n",
            "\n",
            "First 5 rows of the data:\n",
            "   ID number       Date       Location                   Position  \\\n",
            "0          1  22 Apr 08  Palo Alto, CA             Sales Director   \n",
            "1          2  23 Apr 08  Lexington, KY      Systems Administrator   \n",
            "2          3  02 May 08    Seattle, WA           Technical Writer   \n",
            "3          4  23 May 08    Seattle, WA   Software Design Engineer   \n",
            "4          5  25 May 08    Seattle, WA   Senior Marketing Manager   \n",
            "\n",
            "                                 Comment for company  Overall rating  \\\n",
            "0  Amazon isn't all it's cracked up to be, unless...               2   \n",
            "1                            Long hours and low pay.               2   \n",
            "2       A fair website, but sucks as a place to work               2   \n",
            "3  About what you'd expect when you sell your sou...               3   \n",
            "4      I can't believe I get paid to do what I love!               4   \n",
            "\n",
            "   Work/Life Balance  Culture and Values  Diversity and Inclusion  \\\n",
            "0                1.5                 NaN                      NaN   \n",
            "1                2.0                 NaN                      NaN   \n",
            "2                3.0                 NaN                      NaN   \n",
            "3                5.0                 NaN                      NaN   \n",
            "4                5.0                 NaN                      NaN   \n",
            "\n",
            "   Career Opportunities  ...  CEO Approval  Recommended Business Outlook  \\\n",
            "0                   2.0  ...           yes           no              NaN   \n",
            "1                   2.0  ...            no          NaN              NaN   \n",
            "2                   2.0  ...            no           no              NaN   \n",
            "3                   2.0  ...        may be           no              NaN   \n",
            "4                   4.0  ...           yes          yes              NaN   \n",
            "\n",
            "  Current employee Former employee  Timeline  \\\n",
            "0             True           False       NaN   \n",
            "1            False            True       NaN   \n",
            "2             True           False       NaN   \n",
            "3             True           False       NaN   \n",
            "4             True           False       NaN   \n",
            "\n",
            "                                                cons  \\\n",
            "0  The management is arrogant and unprofessional,...   \n",
            "1  60-80 hour work weeks are not uncommon during ...   \n",
            "2  Highly politicized. People are literally not i...   \n",
            "3  Working conditions (small cubes), tools availa...   \n",
            "4  It's tough to stay focused on one career path ...   \n",
            "\n",
            "                                                pros  \\\n",
            "0  It looks good on a resume. The visibility of t...   \n",
            "1  If you take advantage of the discounts, you ca...   \n",
            "2                              You own your own work   \n",
            "3                  Money, Money, Money, Money, Money   \n",
            "4  Amazon.com has endless opportunities, not only...   \n",
            "\n",
            "                                advice to Management  \\\n",
            "0  Take a management course! Where you went to co...   \n",
            "1  Either hire more entry-level people that are q...   \n",
            "2  What do you want to be as a company? How are t...   \n",
            "3  Faster turnaround on evaluations, and put more...   \n",
            "4  Senior Management needs to get in the trenches...   \n",
            "\n",
            "                                          review_url  \n",
            "0  https://www.glassdoor.co.in/Reviews/Employee-R...  \n",
            "1  https://www.glassdoor.co.in/Reviews/Employee-R...  \n",
            "2  https://www.glassdoor.co.in/Reviews/Employee-R...  \n",
            "3  https://www.glassdoor.co.in/Reviews/Employee-R...  \n",
            "4  https://www.glassdoor.co.in/Reviews/Employee-R...  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 28933 entries, 0 to 29493\n",
            "Data columns (total 26 columns):\n",
            " #   Column                     Non-Null Count  Dtype         \n",
            "---  ------                     --------------  -----         \n",
            " 0   ID number                  28933 non-null  int64         \n",
            " 1   Date                       28933 non-null  datetime64[ns]\n",
            " 2   Location                   28933 non-null  object        \n",
            " 3   Position                   28933 non-null  object        \n",
            " 4   Comment for company        28933 non-null  object        \n",
            " 5   Overall rating             28933 non-null  int64         \n",
            " 6   Work/Life Balance          28933 non-null  float64       \n",
            " 7   Culture and Values         28933 non-null  float64       \n",
            " 8   Diversity and Inclusion    28933 non-null  float64       \n",
            " 9   Career Opportunities       28933 non-null  float64       \n",
            " 10  Compensation and Benefits  28933 non-null  float64       \n",
            " 11  Senior Management          28933 non-null  float64       \n",
            " 12  CEO Approval               28933 non-null  object        \n",
            " 13  Recommended                28933 non-null  object        \n",
            " 14  Business Outlook           28933 non-null  object        \n",
            " 15  Current employee           28933 non-null  bool          \n",
            " 16  Former employee            28933 non-null  bool          \n",
            " 17  cons                       28933 non-null  object        \n",
            " 18  pros                       28933 non-null  object        \n",
            " 19  advice to Management       28933 non-null  object        \n",
            " 20  review_url                 28933 non-null  object        \n",
            " 21  Year                       28933 non-null  int64         \n",
            " 22  is_current_employee        28933 non-null  int64         \n",
            " 23  CEO_Approval_Score         28933 non-null  float64       \n",
            " 24  Recommended_Score          28933 non-null  float64       \n",
            " 25  Business_Outlook_Score     28933 non-null  float64       \n",
            "dtypes: bool(2), datetime64[ns](1), float64(9), int64(4), object(10)\n",
            "memory usage: 5.6+ MB\n",
            "Starting analysis and visualizations...\n",
            "\n",
            "Performing Sentiment Analysis with LLM...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0838929f75ff402d9d39cbf1e6cadc80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Getting Llama Sentiments (Batched):   0%|          | 0/905 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e13f7c990c9b4f87a161247d68cb38b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Getting Llama Sentiments (Batched):   0%|          | 0/905 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    }
  ]
}